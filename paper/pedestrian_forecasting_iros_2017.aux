\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{Helbing1992}
\citation{Ziebart2008,Ziebart2009,Kitani2012,Xie2013,Karasev2016}
\citation{Kitani2012}
\citation{Kitani2012}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}{section.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The performance of the presented algorithm captures the most probable routes that a pedestrian chooses. The dot is the starting point of the trajectory, the diamond is the position at time $t$, and the X is the end of the trajectory. The likelihood of detection is depicted using the \textit  {virdis} color palette. The presented algorithm took $0.00465$s per frame in a Python implementation. \relax }}{1}{figure.caption.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:gates-1-2}{{1}{1}{The performance of the presented algorithm captures the most probable routes that a pedestrian chooses. The dot is the starting point of the trajectory, the diamond is the position at time $t$, and the X is the end of the trajectory. The likelihood of detection is depicted using the \textit {virdis} color palette. The presented algorithm took $0.00465$s per frame in a Python implementation. \relax }{figure.caption.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {I-A}Background}{1}{subsection.1.1}}
\citation{Karasev2016}
\citation{Ballan2016}
\citation{Karasev2016}
\citation{Ballan2016}
\citation{Walker2014}
\citation{Walker2014}
\citation{Kitani2012}
\citation{Karasev2016,Ballan2016}
\citation{Helbing1995,Xu2012}
\citation{Helbing1995}
\citation{Pellegrini2009,Yamaguchi2011,Yi2016}
\citation{Hospedales2009,Wang2009,Emonet2011}
\citation{Koppula2016}
\citation{Tay2008,Wang2008,Trautman2015}
\citation{Alahi2016}
\citation{Robicquet2016}
\newmarginnote{note.2.1}{{2}{3552215sp}}
\@writefile{toc}{\contentsline {subsection}{\numberline {I-B}Contributions}{2}{subsection.1.2}}
\@writefile{toc}{\contentsline {section}{\numberline {II}Model}{2}{section.2}}
\newlabel{sec:model}{{II}{2}{Model}{section.2}{}}
\citation{MTA}
\@writefile{toc}{\contentsline {subsection}{\numberline {II-A}The Variables of the Model}{3}{subsection.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {II-B}The Sensor Model}{3}{subsection.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {II-C}The Agent Model}{3}{subsection.2.3}}
\newlabel{eq:ode}{{2}{3}{The Agent Model}{equation.2.2}{}}
\newlabel{eq:x_check | ksx}{{3}{3}{The Agent Model}{equation.2.3}{}}
\newlabel{eq:v | ksx}{{4}{3}{The Agent Model}{equation.2.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {II-D}The Full Model}{3}{subsection.2.4}}
\newlabel{eq:pgm}{{5}{3}{The Full Model}{equation.2.5}{}}
\newlabel{eq:decomposition}{{7}{3}{The Full Model}{equation.2.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {III}Efficient Probability Propagation}{3}{section.3}}
\newlabel{sec:efficient}{{III}{3}{Efficient Probability Propagation}{section.3}{}}
\citation{Leveque1992,Gottlieb2001}
\newlabel{eq:approximation 0}{{8}{4}{Efficient Probability Propagation}{equation.3.8}{}}
\newlabel{eq:convolve}{{9}{4}{Efficient Probability Propagation}{equation.3.9}{}}
\newlabel{eq:push forward}{{12}{4}{Efficient Probability Propagation}{equation.3.12}{}}
\newlabel{eq:approximation 1}{{13}{4}{Efficient Probability Propagation}{equation.3.13}{}}
\newlabel{eq:makesense}{{14}{4}{Efficient Probability Propagation}{equation.3.14}{}}
\newlabel{thm:error}{{1}{4}{Efficient Probability Propagation}{thm.3.1}{}}
\newlabel{cor:error}{{1}{4}{Efficient Probability Propagation}{cor.1}{}}
\newlabel{eq:approximation 2}{{15}{4}{Efficient Probability Propagation}{equation.3.15}{}}
\newlabel{thm:symmetry}{{2}{4}{Efficient Probability Propagation}{thm.3.2}{}}
\citation{Robicquet2016}
\citation{FreyDueck2007}
\citation{Morris2009}
\citation{Lee2007}
\newlabel{thm:main}{{3}{5}{Efficient Probability Propagation}{thm.3.3}{}}
\newlabel{eq:approximation 3}{{16}{5}{Efficient Probability Propagation}{equation.3.16}{}}
\newlabel{eq:partition}{{17}{5}{Efficient Probability Propagation}{equation.3.17}{}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Implementation and Experimental results}{5}{section.4}}
\newlabel{sec:implementation}{{IV}{5}{Implementation and Experimental results}{section.4}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Algorithm to compute $\rho _t$ for each $t \in \{\Delta t, 2 \Delta t, \dots  , N_t \Delta t$\}.\relax }}{5}{algorithm.1}}
\newlabel{alg:1}{{1}{5}{Algorithm to compute $\rho _t$ for each $t \in \{\Delta t, 2 \Delta t, \dots , N_t \Delta t$\}.\relax }{algorithm.1}{}}
\newlabel{alg1:step2}{{6}{5}{Algorithm to compute $\rho _t$ for each $t \in \{\Delta t, 2 \Delta t, \dots , N_t \Delta t$\}.\relax }{algorithm.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {IV-A}Learning the Vector Fields}{5}{subsection.4.1}}
\newmarginnote{note.5.1}{{5}{22542751sp}}
\citation{Kitani2012}
\citation{Alahi2016}
\citation{Kitani2012}
\citation{Alahi2016}
\citation{Kitani2012}
\citation{Alahi2016}
\citation{Robicquet2016}
\citation{Kitani2012}
\@writefile{toc}{\contentsline {subsection}{\numberline {IV-B}Learning $\qopname  \relax m{Pr}( x_0 \mid M)$ and $\qopname  \relax m{Pr}(M)$}{6}{subsection.4.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {IV-C}Learning the Measurement Model}{6}{subsection.4.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {IV-D}Learning the Noise Model}{6}{subsection.4.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {IV-E}Evaluating Performance}{6}{subsection.4.5}}
\newmarginnote{note.6.2}{{6}{26262504sp}}
\newmarginnote{note.6.3}{{6}{29286080sp}}
\newmarginnote{note.6.4}{{6}{29109346sp}}
\citation{Kitani2012}
\citation{Kitani2012}
\citation{Alahi2016}
\citation{Kitani2012}
\citation{Alahi2016}
\citation{Alahi2016}
\citation{Kitani2012}
\citation{Alahi2016}
\citation{Kitani2012}
\citation{Kitani2012}
\newlabel{fig:bookstore-1-2}{{2a}{7}{Scene 1\relax }{figure.caption.2}{}}
\newlabel{sub@fig:bookstore-1-2}{{a}{7}{Scene 1\relax }{figure.caption.2}{}}
\newlabel{fig:death-1-2}{{2b}{7}{Scene 2\relax }{figure.caption.2}{}}
\newlabel{sub@fig:death-1-2}{{b}{7}{Scene 2\relax }{figure.caption.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces An illustration of the predictions generated by the various algorithms. In this figure, the dot is the start point of the test trajectory, the diamond is the position at time t, and the X is the end of the trajectory. The likelihood of detection is depicted using the virdis color palette. Notice that the Random Walk is imprecise, while the predictions generated by the algorithm in \cite  {Kitani2012} are unable to match the speed of the agent and choose the wrong direction to follow the agent around the circle. The algorithm in \cite  {Alahi2016} is confident and close to the trajectory at small times, but their lack of a motion model causes their prediction to compress into a point at intermediate time scales.\relax }}{7}{figure.caption.2}}
\newmarginnote{note.6.1}{{7}{3552215sp}}
\newmarginnote{note.7.3}{{7}{3552215sp}}
\@writefile{lot}{\contentsline {table}{\numberline {I}{\ignorespaces Comparison of runtimes of the various algorithms.\relax }}{7}{table.caption.3}}
\newlabel{tab:time}{{I}{7}{Comparison of runtimes of the various algorithms.\relax }{table.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces \leavevmode {\color  {red}A comparison of the AUC of the various algorithms. Note that the initial dip in the performance of \cite  {Kitani2012} is due to their confidence in their initial estimate. We sampled the S-LSTM \cite  {Alahi2016} model $100$ times to extract a less concentrated probability distribution from their algorithm.}\relax }}{7}{figure.caption.4}}
\newmarginnote{note.7.1}{{7}{20602840sp}}
\newlabel{fig:auc_vs_time}{{3}{7}{\rtext {A comparison of the AUC of the various algorithms. Note that the initial dip in the performance of \cite {Kitani2012} is due to their confidence in their initial estimate. We sampled the S-LSTM \cite {Alahi2016} model $100$ times to extract a less concentrated probability distribution from their algorithm.}\relax }{figure.caption.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {V}Conclusion}{7}{section.5}}
\newlabel{sec:conclusion}{{V}{7}{Conclusion}{section.5}{}}
\citation{Helbing1995}
\citation{Ballan2016}
\bibstyle{IEEEtran}
\bibdata{references}
\bibcite{Helbing1992}{1}
\bibcite{Ziebart2008}{2}
\bibcite{Ziebart2009}{3}
\bibcite{Kitani2012}{4}
\bibcite{Xie2013}{5}
\bibcite{Karasev2016}{6}
\bibcite{Ballan2016}{7}
\bibcite{Walker2014}{8}
\bibcite{Helbing1995}{9}
\bibcite{Xu2012}{10}
\bibcite{Pellegrini2009}{11}
\bibcite{Yamaguchi2011}{12}
\bibcite{Yi2016}{13}
\bibcite{Hospedales2009}{14}
\bibcite{Wang2009}{15}
\bibcite{Emonet2011}{16}
\bibcite{Koppula2016}{17}
\bibcite{Tay2008}{18}
\bibcite{Wang2008}{19}
\bibcite{Trautman2015}{20}
\bibcite{Alahi2016}{21}
\bibcite{Robicquet2016}{22}
\bibcite{MTA}{23}
\bibcite{Leveque1992}{24}
\bibcite{Gottlieb2001}{25}
\bibcite{FreyDueck2007}{26}
\bibcite{Morris2009}{27}
\bibcite{Lee2007}{28}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces \leavevmode {\color  {red}A comparison of the MHD from the ground truth of the pedestrian to a 1000 point samples from each distribution. The method from \cite  {Alahi2016} does well at short time scales since it places a highly confident distribution at the given initial position of the pedestrian, but the method developed in this paper outperforms all others at intermediate times. \cite  {Kitani2012}, which requires the end point of each trajectory, outperforms all other algorithms at longer time scales since they assume that the end point of the trajectory is unknown.}\relax }}{8}{figure.caption.5}}
\newmarginnote{note.7.2}{{8}{3552215sp}}
\newlabel{fig:mhd_vs_time}{{4}{8}{\rtext {A comparison of the MHD from the ground truth of the pedestrian to a 1000 point samples from each distribution. The method from \cite {Alahi2016} does well at short time scales since it places a highly confident distribution at the given initial position of the pedestrian, but the method developed in this paper outperforms all others at intermediate times. \cite {Kitani2012}, which requires the end point of each trajectory, outperforms all other algorithms at longer time scales since they assume that the end point of the trajectory is unknown.}\relax }{figure.caption.5}{}}
\newmarginnote{note.8.1}{{8}{3552215sp}}
\newmarginnote{note.8.2}{{8}{7881645sp}}
\@writefile{toc}{\contentsline {section}{References}{8}{section*.6}}
