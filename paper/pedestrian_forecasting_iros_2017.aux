\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{Helbing1992}
\citation{kalman1960new}
\citation{Schneider2013}
\citation{Kooji2014}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}{section.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The performance of the presented algorithm captures the most probable routes that a pedestrian chooses. In this figure, the dot is the start point of the test trajectory, the diamond is the position at time $t$, and the X is the end of the trajectory. The likelihood of detection is depicted using the \textit  {virdis} color palette. In this example which was captured at thirty frames per second, the presented algorithm took 0.00465s per frame in a Python implementation. \relax }}{1}{figure.caption.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:gates-1-2}{{1}{1}{The performance of the presented algorithm captures the most probable routes that a pedestrian chooses. In this figure, the dot is the start point of the test trajectory, the diamond is the position at time $t$, and the X is the end of the trajectory. The likelihood of detection is depicted using the \textit {virdis} color palette. In this example which was captured at thirty frames per second, the presented algorithm took 0.00465s per frame in a Python implementation. \relax }{figure.caption.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {I-A}Background}{1}{subsection.1.1}}
\citation{Ziebart2008}
\citation{Ziebart2009}
\citation{Kitani2012}
\citation{Xie2013}
\citation{Karasev2016}
\citation{Kitani2012}
\citation{Kitani2012}
\citation{Kitani2012}
\citation{Karasev2016}
\citation{Karasev2016}
\citation{Doucet2000}
\citation{Ballan2016}
\citation{Karasev2016}
\citation{Ballan2016}
\citation{Ballan2016}
\citation{Walker2014}
\citation{Walker2014}
\citation{Kitani2012}
\citation{Walker2014}
\citation{Karasev2016}
\citation{Ballan2016}
\citation{Helbing1995}
\citation{Xu2012}
\citation{Yamaguchi2011}
\citation{Yi2016}
\citation{Pellegrini2009}
\citation{Helbing1995}
\citation{Emonet2011}
\citation{Hospedales2009}
\citation{Wang2009}
\citation{Wang2009}
\citation{Emonet2011}
\citation{Hospedales2009}
\citation{Koppula2016}
\citation{Tay2008}
\citation{Wang2008}
\citation{Trautman2015}
\citation{Alahi2016}
\citation{Kitani2012}
\citation{Karasev2016}
\newmarginnote{note.2.1}{{2}{20602840sp}}
\@writefile{toc}{\contentsline {subsection}{\numberline {I-B}Contributions}{2}{subsection.1.2}}
\citation{Robicquet2016}
\citation{MTA}
\@writefile{toc}{\contentsline {section}{\numberline {II}Model}{3}{section.2}}
\newlabel{sec:model}{{II}{3}{Model}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {II-A}The Variables of the Model}{3}{subsection.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {II-B}The Sensor Model}{3}{subsection.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {II-C}The Agent Model}{3}{subsection.2.3}}
\newlabel{eq:ode}{{2}{3}{The Agent Model}{equation.2.2}{}}
\newlabel{eq:x_check | ksx}{{3}{3}{The Agent Model}{equation.2.3}{}}
\newlabel{eq:v | ksx}{{4}{3}{The Agent Model}{equation.2.4}{}}
\newlabel{eq:models}{{5}{4}{The Agent Model}{equation.2.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {II-D}The Full Model}{4}{subsection.2.4}}
\newlabel{eq:pgm}{{6}{4}{The Full Model}{equation.2.6}{}}
\newlabel{eq:decomposition}{{8}{4}{The Full Model}{equation.2.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {III}Efficient Probability Propagation}{4}{section.3}}
\newlabel{sec:efficient}{{III}{4}{Efficient Probability Propagation}{section.3}{}}
\newlabel{eq:approximation 0}{{9}{4}{Efficient Probability Propagation}{equation.3.9}{}}
\newlabel{eq:convolve}{{12}{4}{Efficient Probability Propagation}{equation.3.12}{}}
\newlabel{eq:push forward}{{17}{4}{Efficient Probability Propagation}{equation.3.17}{}}
\citation{Leveque1992}
\citation{Gottlieb2001}
\newlabel{eq:approximation 1}{{18}{5}{Efficient Probability Propagation}{equation.3.18}{}}
\newlabel{eq:makesense}{{19}{5}{Efficient Probability Propagation}{equation.3.19}{}}
\newlabel{thm:error}{{1}{5}{Efficient Probability Propagation}{thm.3.1}{}}
\newlabel{cor:error}{{1}{5}{Efficient Probability Propagation}{cor.1}{}}
\newlabel{eq:approximation 2}{{20}{5}{Efficient Probability Propagation}{equation.3.20}{}}
\newlabel{thm:symmetry}{{2}{5}{Efficient Probability Propagation}{thm.3.2}{}}
\newlabel{thm:main}{{3}{5}{Efficient Probability Propagation}{thm.3.3}{}}
\newlabel{eq:approximation 3}{{25}{5}{Efficient Probability Propagation}{equation.3.25}{}}
\citation{Robicquet2016}
\citation{FreyDueck2007}
\citation{Morris2009}
\citation{Lee2007}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Algorithm to compute $\rho _t$ for each $t \in \{\Delta t, 2 \Delta t, \dots  , N_t \Delta t$\}.\relax }}{6}{algorithm.1}}
\newlabel{alg:1}{{1}{6}{Algorithm to compute $\rho _t$ for each $t \in \{\Delta t, 2 \Delta t, \dots , N_t \Delta t$\}.\relax }{algorithm.1}{}}
\newlabel{alg1:step2}{{6}{6}{Algorithm to compute $\rho _t$ for each $t \in \{\Delta t, 2 \Delta t, \dots , N_t \Delta t$\}.\relax }{algorithm.1}{}}
\newlabel{eq:partition}{{26}{6}{Efficient Probability Propagation}{equation.3.26}{}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Implementation and Experimental results}{6}{section.4}}
\newlabel{sec:implementation}{{IV}{6}{Implementation and Experimental results}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {IV-A}Learning the Vector Fields}{6}{subsection.4.1}}
\newmarginnote{note.6.1}{{6}{22707277sp}}
\citation{Kitani2012}
\citation{Robicquet2016}
\@writefile{toc}{\contentsline {subsection}{\numberline {IV-B}Learning $\qopname  \relax m{Pr}( x_0 \mid M)$ and $\qopname  \relax m{Pr}(M)$}{7}{subsection.4.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {IV-C}Learning the Measurement Model}{7}{subsection.4.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces  \leavevmode {\color  {red}An illustration of the predictions generated by the various algorithms. In this figure, the dot is the start point of the test trajectory, the diamond is the position at time t, and the X is the end of the trajectory. The likelihood of detection is depicted using the virdis color palette. Notice that the Random Walk is imprecise, while the predictions generated by the algorithm in [7] suffer from the inability of their motion model to adequately match the speed of the agent. The algorithm in [13] is confident and close to the trajectory at small times, but their lack of a motion model causes their prediction to compress into a point at intermediate time scales.}\relax }}{7}{figure.caption.2}}
\newmarginnote{note.7.1}{{7}{20602840sp}}
\newlabel{fig:bookstore-1-2}{{2}{7}{\rtext {An illustration of the predictions generated by the various algorithms. In this figure, the dot is the start point of the test trajectory, the diamond is the position at time t, and the X is the end of the trajectory. The likelihood of detection is depicted using the virdis color palette. Notice that the Random Walk is imprecise, while the predictions generated by the algorithm in [7] suffer from the inability of their motion model to adequately match the speed of the agent. The algorithm in [13] is confident and close to the trajectory at small times, but their lack of a motion model causes their prediction to compress into a point at intermediate time scales.}\relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {IV-D}Learning the Noise Model}{7}{subsection.4.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {IV-E}Evaluating Performance}{7}{subsection.4.5}}
\newmarginnote{note.7.3}{{7}{34811829sp}}
\citation{Kitani2012}
\citation{Kitani2012}
\citation{Kitani2012}
\citation{Alahi2016}
\citation{Kitani2012}
\citation{Alahi2016}
\citation{Alahi2016}
\citation{Kitani2012}
\citation{Alahi2016}
\citation{Alahi2016}
\citation{Kitani2012}
\citation{Alahi2016}
\citation{Kitani2012}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces \leavevmode {\color  {red}An illustration of the predictions generated by the various algorithms. In this figure, the dot is the start point of the test trajectory, the diamond is the position at time t, and the X is the end of the trajectory. The likelihood of detection is depicted using the virdis color palette. Notice that the Random Walk is imprecise, while the predictions generated by the algorithm in [7] are unable to match the speed of the agent and choose the wrong direction to follow the agent around the circle. The algorithm in [13] is confident and close to the trajectory at small times, but their lack of a motion model causes their prediction to compress into a point at intermediate time scales.}\relax }}{8}{figure.caption.3}}
\newmarginnote{note.7.2}{{8}{3552215sp}}
\newlabel{fig:death-1-2}{{3}{8}{\rtext {An illustration of the predictions generated by the various algorithms. In this figure, the dot is the start point of the test trajectory, the diamond is the position at time t, and the X is the end of the trajectory. The likelihood of detection is depicted using the virdis color palette. Notice that the Random Walk is imprecise, while the predictions generated by the algorithm in [7] are unable to match the speed of the agent and choose the wrong direction to follow the agent around the circle. The algorithm in [13] is confident and close to the trajectory at small times, but their lack of a motion model causes their prediction to compress into a point at intermediate time scales.}\relax }{figure.caption.3}{}}
\newmarginnote{note.7.4}{{8}{3552215sp}}
\newmarginnote{note.8.1}{{8}{19109685sp}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces \leavevmode {\color  {red}A comparison of the AUC of the various algorithms. Note that the initial dip in the performance of \cite  {Kitani2012} is due to their confidence in their initial estimate. We sampled the S-LSTM \cite  {Alahi2016} model 100 times in order to give them the best chance in this analysis, but their confidence combined with the over-reliance on ourdated pedestrian measurements at moderate-to-large timescales lowered their performance.}\relax }}{8}{figure.caption.4}}
\newmarginnote{note.8.2}{{8}{20602840sp}}
\newlabel{fig:auc_vs_time}{{4}{8}{\rtext {A comparison of the AUC of the various algorithms. Note that the initial dip in the performance of \cite {Kitani2012} is due to their confidence in their initial estimate. We sampled the S-LSTM \cite {Alahi2016} model 100 times in order to give them the best chance in this analysis, but their confidence combined with the over-reliance on ourdated pedestrian measurements at moderate-to-large timescales lowered their performance.}\relax }{figure.caption.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {I}{\ignorespaces Comparison of runtimes of the various algorithms.\relax }}{8}{table.caption.6}}
\newlabel{tab:time}{{I}{8}{Comparison of runtimes of the various algorithms.\relax }{table.caption.6}{}}
\newmarginnote{note.8.4}{{8}{20602840sp}}
\citation{Helbing1995}
\citation{Walker2014}
\citation{Ballan2016}
\citation{Ballan2016}
\bibstyle{IEEEtran}
\bibdata{references}
\bibcite{Helbing1992}{1}
\bibcite{kalman1960new}{2}
\bibcite{Schneider2013}{3}
\bibcite{Kooji2014}{4}
\bibcite{Ziebart2008}{5}
\bibcite{Ziebart2009}{6}
\bibcite{Kitani2012}{7}
\bibcite{Xie2013}{8}
\bibcite{Karasev2016}{9}
\bibcite{Doucet2000}{10}
\bibcite{Ballan2016}{11}
\bibcite{Walker2014}{12}
\bibcite{Helbing1995}{13}
\bibcite{Xu2012}{14}
\bibcite{Yamaguchi2011}{15}
\bibcite{Yi2016}{16}
\bibcite{Pellegrini2009}{17}
\bibcite{Emonet2011}{18}
\bibcite{Hospedales2009}{19}
\bibcite{Wang2009}{20}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces \leavevmode {\color  {red}A comparison of the Modified Hausdorff Distance from the ground truth of the pedestrian to a 1000 point sample from each distribution. The method from \cite  {Alahi2016} does very well at short time scales due to its confidence, but we outperform all other methods at intermediate times. At long timescales the MHD to the trajectory of most algorithms converges. Our method increases positional uncertainty with time, and so \cite  {Kitani2012} outperforms us because they know the end point and we do not, and \cite  {Alahi2016} places too much significance on the initial positions of other pedestrians at large time scales.}\relax }}{9}{figure.caption.5}}
\newmarginnote{note.8.3}{{9}{3552215sp}}
\newlabel{fig:mhd_vs_time}{{5}{9}{\rtext {A comparison of the Modified Hausdorff Distance from the ground truth of the pedestrian to a 1000 point sample from each distribution. The method from \cite {Alahi2016} does very well at short time scales due to its confidence, but we outperform all other methods at intermediate times. At long timescales the MHD to the trajectory of most algorithms converges. Our method increases positional uncertainty with time, and so \cite {Kitani2012} outperforms us because they know the end point and we do not, and \cite {Alahi2016} places too much significance on the initial positions of other pedestrians at large time scales.}\relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {V}Conclusion}{9}{section.5}}
\newlabel{sec:conclusion}{{V}{9}{Conclusion}{section.5}{}}
\newmarginnote{note.9.1}{{9}{5285473sp}}
\newmarginnote{note.9.2}{{9}{12161607sp}}
\@writefile{toc}{\contentsline {section}{References}{9}{section*.8}}
\bibcite{Koppula2016}{21}
\bibcite{Tay2008}{22}
\bibcite{Wang2008}{23}
\bibcite{Trautman2015}{24}
\bibcite{Alahi2016}{25}
\bibcite{Robicquet2016}{26}
\bibcite{MTA}{27}
\bibcite{Leveque1992}{28}
\bibcite{Gottlieb2001}{29}
\bibcite{FreyDueck2007}{30}
\bibcite{Morris2009}{31}
\bibcite{Lee2007}{32}
